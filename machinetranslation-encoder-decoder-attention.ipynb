{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4246862,"sourceType":"datasetVersion","datasetId":2502545}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=toc></a>\n<h1 style=\"padding: 35px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;background-position: 0px 0px; \n\"><span style='color:white'><b> Machine Translation Using Encoder-Decoder with Attention Mechanism</b></span></h1>\n\n<br>\n\n<center>\n    <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20231226141038/Machine-Translation-Model.png\" \n         alt =\"Machine Translation\" \n         style='width: 60%;'>\n    <figcaption>\n            Source: <a href='https://media.geeksforgeeks.org/wp-content/uploads/20231226141038/Machine-Translation-Model.png'> GFG | Machine Translation</a>\n    </figcaption>\n</center>\n\n## üéØ Objective\n The goal of this notebook is to practice Encoder-Decoder architecture by doing Machine translation. In this notebook I am doing English to hindi translation using Encoder-Decoder .\n\n## üìÅ Dataset\nThe dataset used in this notebook is the '<b>IIT Bombay English-Hindi Translation Dataset</b>' uploaded by @VAIBHAV KUMAR on kaggle. This Data contains 1,561,840 instances of Hindi - English Translation\n\n<br>\n\n<hr>\n\n## Table of contents\n- [1 | Overview  of working technology](#1)\n\n- [2 | Importing Required Libraries & DataSource](#2)\n   > - [Load & Inspect Data](#2.1)\n \n- [3 | Data Exploration](#3)\n   > - [Computing Dimension of Dataset](#3.1)\n   > - [Statistical Summary of Dataset](#3.2)\n   > - [Checking if There's Any Duplicate Records](#3.3)\n   > - [Computing Total No. of Missing Values and the Percentage of Missing Values](#3.4)\n   > - [Performing Descriptive Analysis](#3.5)\n   \n   \n- [4 | Preprocessing](#4) \n   > - [Dropping Duplicates and Null Values](#4.1)\n   > - [Lowercasing](#4.2)\n   > - [Removing Html tags](#4.3)\n   > - [Removing URLs](#4.4)\n   > - [Chat word treatment](#4.5)\n   > - [Removing Emojis](#4.6)\n   > - [Removing contractions](#4.7)\n   > - [Remove Non-Hindi and Alphanumeric characters](#4.8)\n   > - [WordCloud](#4.9)\n   > - [Tokenize on the Data ](#4.10)\n   > - [Split the Data into Independent and Dependent Variable](#4.11)\n   > - [Padding the Data](#4.12)\n   > - [Train Test and split](#4.13)\n   \n- [5 | Modelling](#5)\n   > - [Enocer Decoder model](#5.1)\n   > - [Enocer Decoder model Evaluation](#5.2)\n   > - [Prediction from Enocer Decoder model](#5.3)\n   > - [Enocer Decoder model with attention](#5.4)\n   > - [Enocer Decoder model's with attention Evaluation](#5.5)\n   > - [Prediction from Enocer Decoder model with attention](#5.6)\n   > - [Saving the weights](#5.7)\n\n<br>\n\n<hr>\n\n# Author Details:\n- **Name: Anmol Gupta**\n- **Email: getanmol.gupta@gmail.com**\n- **LinkedIn: www.linkedin.com/in/anmol-gupta-9a905a228**\n- **GitHub: https://github.com/anmolgupta01**","metadata":{}},{"cell_type":"markdown","source":"<a id='1'></a>\n# 1 | Overview of Working Technology\n\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;\"></div>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=toc></a>\n<h1 style=\"padding: 35px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;background-position: 0px 0px; \n\"><span style='color:white'><b> Ecoder Decoder Architecture without Attention </b></span></h1>","metadata":{}},{"cell_type":"markdown","source":"![Encoder](https://6chaoran.wordpress.com/wp-content/uploads/2019/01/encoder-decoder-architecture.png?w=640)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T14:00:29.688355Z","iopub.execute_input":"2024-07-07T14:00:29.688861Z","iopub.status.idle":"2024-07-07T14:00:30.908182Z","shell.execute_reply.started":"2024-07-07T14:00:29.688826Z","shell.execute_reply":"2024-07-07T14:00:30.906490Z"}}},{"cell_type":"markdown","source":"<a id=toc></a>\n<h1 style=\"padding: 35px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;background-position: 0px 0px; \n\"><span style='color:white'><b> Ecoder Decoder Architecture with Attention </b></span></h1>","metadata":{}},{"cell_type":"markdown","source":"![Encoder](https://www.researchgate.net/publication/377750342/figure/fig2/AS:11431281220369226@1706414219715/RNN-encoder-decoder-with-an-Attention-mechanism.png)","metadata":{}},{"cell_type":"markdown","source":"### Encoder-Decoder Architecture in RNNs:\n- **Architecture**: \n  - Comprises two RNNs: an Encoder and a Decoder.\n  - Encoder processes the input sequence and converts it into a fixed-size context vector.\n  - Decoder takes this context vector and generates the output sequence step by step.\n\n- **Encoder**:\n  - Takes input sequence one token at a time.\n  - Outputs a context vector summarizing the input sequence.\n  - Can use various RNN variants like LSTM or GRU.\n\n- **Decoder**:\n  - Takes the context vector from the Encoder and generates the output sequence.\n  - Processes the output sequence step by step, incorporating information from the context vector.\n  - Can have a different architecture from the Encoder, but typically uses the same RNN variant.\n\n- **Training**:\n  - During training, both Encoder and Decoder are jointly trained using teacher forcing.\n  - Teacher forcing involves providing the correct previous target token as input to the Decoder at each step.","metadata":{}},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#78D1E1;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n\n<a id='2'></a>\n# 2 | Importing Required Libraries & DataSource\n\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;\"></div>\n","metadata":{}},{"cell_type":"code","source":"!pip install contractions","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:16:43.487143Z","iopub.execute_input":"2024-09-19T09:16:43.487561Z","iopub.status.idle":"2024-09-19T09:16:57.735388Z","shell.execute_reply.started":"2024-09-19T09:16:43.487531Z","shell.execute_reply":"2024-09-19T09:16:57.734455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textblob import Word\nimport nltk\nnltk.data.path.append(\"/kaggle/working/\")\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:16:57.737452Z","iopub.execute_input":"2024-09-19T09:16:57.737719Z","iopub.status.idle":"2024-09-19T09:16:59.070203Z","shell.execute_reply.started":"2024-09-19T09:16:57.737694Z","shell.execute_reply":"2024-09-19T09:16:59.069326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Libraray for Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Libraray for Data Visualisation\nimport seaborn as sns \nimport matplotlib.pyplot as plt \nsns.set(style=\"white\",font_scale=1.5)\nsns.set(rc={\"axes.facecolor\":\"#FFFAF0\",\"figure.facecolor\":\"#FFFAF0\"})\nsns.set_context(\"poster\",font_scale = .7)\n\n# Library for preprocessing \nimport contractions\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nimport spacy\nimport math\nfrom gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom joblib import Parallel, delayed\nimport re,string,unicodedata\nfrom sklearn.model_selection import train_test_split\nfrom textblob import TextBlob\nfrom scipy.sparse import lil_matrix\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom nltk import pos_tag\n\n# Library for model building\nfrom tensorflow.keras.regularizers import l2\nimport tensorflow\nimport keras\nfrom keras import layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential,Model\n#from attention import BahdanauAttention\nfrom keras.layers import SimpleRNN,LSTM,GRU, Embedding, Dense, SpatialDropout1D, Dropout, BatchNormalization, Bidirectional, Attention, Input\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.initializers import GlorotUniform\nfrom tensorflow.keras.optimizers import Adam\n\n#Library to overcome Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:16:59.071411Z","iopub.execute_input":"2024-09-19T09:16:59.071721Z","iopub.status.idle":"2024-09-19T09:17:33.813287Z","shell.execute_reply.started":"2024-09-19T09:16:59.071694Z","shell.execute_reply":"2024-09-19T09:17:33.812287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#78D1E1;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n\n<a id='1.1'></a>\n# Load and Inspect Dataset\n\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;\"></div>\n","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/english-hindi-dataset/Dataset_English_Hindi.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:33.814664Z","iopub.execute_input":"2024-09-19T09:17:33.815526Z","iopub.status.idle":"2024-09-19T09:17:35.050036Z","shell.execute_reply.started":"2024-09-19T09:17:33.815486Z","shell.execute_reply":"2024-09-19T09:17:35.049232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:35.052426Z","iopub.execute_input":"2024-09-19T09:17:35.052729Z","iopub.status.idle":"2024-09-19T09:17:35.077998Z","shell.execute_reply.started":"2024-09-19T09:17:35.052704Z","shell.execute_reply":"2024-09-19T09:17:35.077024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#78D1E1;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n\n<a id='3'></a>\n# 3 | Data Exploration\n\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;\"></div>\n","metadata":{}},{"cell_type":"markdown","source":"<a id='3.1'></a>\n#### 3.1. Computing Dimension of Dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-07T14:15:32.368708Z","iopub.execute_input":"2024-07-07T14:15:32.369273Z","iopub.status.idle":"2024-07-07T14:15:32.379257Z","shell.execute_reply.started":"2024-07-07T14:15:32.369235Z","shell.execute_reply":"2024-07-07T14:15:32.377314Z"}}},{"cell_type":"code","source":"print(\"dataset shape: \",data.shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:35.079207Z","iopub.execute_input":"2024-09-19T09:17:35.079501Z","iopub.status.idle":"2024-09-19T09:17:35.130802Z","shell.execute_reply.started":"2024-09-19T09:17:35.079476Z","shell.execute_reply":"2024-09-19T09:17:35.130033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 dashed; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown> üîç Inference:</font></h3>\n\n* There is total **1561841 records** and **2 columns** availabe in the data.","metadata":{}},{"cell_type":"markdown","source":"<a id='3.2'></a>\n#### 3.2. Statistical Summary of Dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-07T14:16:38.503878Z","iopub.execute_input":"2024-07-07T14:16:38.504356Z","iopub.status.idle":"2024-07-07T14:16:38.512008Z","shell.execute_reply.started":"2024-07-07T14:16:38.504322Z","shell.execute_reply":"2024-07-07T14:16:38.510486Z"}}},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:35.131893Z","iopub.execute_input":"2024-09-19T09:17:35.132233Z","iopub.status.idle":"2024-09-19T09:17:35.182475Z","shell.execute_reply.started":"2024-09-19T09:17:35.132201Z","shell.execute_reply":"2024-09-19T09:17:35.181595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='3.3'></a>\n#### 3.3. Checking if There's Any Duplicate Records.","metadata":{"execution":{"iopub.status.busy":"2024-07-07T14:17:27.248099Z","iopub.execute_input":"2024-07-07T14:17:27.249131Z","iopub.status.idle":"2024-07-07T14:17:27.255988Z","shell.execute_reply.started":"2024-07-07T14:17:27.249087Z","shell.execute_reply":"2024-07-07T14:17:27.254528Z"}}},{"cell_type":"code","source":"print(\"Duplicates in Dataset: \",data.duplicated().sum())","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:35.183669Z","iopub.execute_input":"2024-09-19T09:17:35.184214Z","iopub.status.idle":"2024-09-19T09:17:35.279512Z","shell.execute_reply.started":"2024-09-19T09:17:35.184179Z","shell.execute_reply":"2024-09-19T09:17:35.278585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='3.4'></a>\n#### 3.4. Computing Total No. of Missing Values and the Percentage of Missing Values","metadata":{}},{"cell_type":"code","source":"missing_data = data.isnull().sum().to_frame().rename(columns={0:\"Total No. of Missing Values\"})\nmissing_data[\"% of Missing Values\"] = round((missing_data[\"Total No. of Missing Values\"]/len(data))*100,2)\nmissing_data","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:35.280805Z","iopub.execute_input":"2024-09-19T09:17:35.281159Z","iopub.status.idle":"2024-09-19T09:17:35.321891Z","shell.execute_reply.started":"2024-09-19T09:17:35.281133Z","shell.execute_reply":"2024-09-19T09:17:35.320939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 dashed; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown> üîç Inference:</font></h3>\n\n* Some of the Attribute are having Missing Values.  ","metadata":{}},{"cell_type":"markdown","source":"<a id='3.5'></a>\n#### 3.5. Performing Descriptive Analysis","metadata":{}},{"cell_type":"code","source":"round(data.describe().T,2)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:35.323315Z","iopub.execute_input":"2024-09-19T09:17:35.323642Z","iopub.status.idle":"2024-09-19T09:17:35.515213Z","shell.execute_reply.started":"2024-09-19T09:17:35.323617Z","shell.execute_reply":"2024-09-19T09:17:35.514303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#78D1E1;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n\n<a id='4'></a>\n# 4 | Preprocessing\n\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;\"></div>\n","metadata":{}},{"cell_type":"markdown","source":"<a id='4.1'></a>\n### **1. Dropping Duplicates and Null Values**","metadata":{}},{"cell_type":"code","source":"data.drop_duplicates(inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:35.516699Z","iopub.execute_input":"2024-09-19T09:17:35.517120Z","iopub.status.idle":"2024-09-19T09:17:35.597290Z","shell.execute_reply.started":"2024-09-19T09:17:35.517084Z","shell.execute_reply":"2024-09-19T09:17:35.596462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dropna(inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:35.598374Z","iopub.execute_input":"2024-09-19T09:17:35.598673Z","iopub.status.idle":"2024-09-19T09:17:35.633728Z","shell.execute_reply.started":"2024-09-19T09:17:35.598647Z","shell.execute_reply":"2024-09-19T09:17:35.632889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4.2'></a>\n### **2. Lowercasing**","metadata":{}},{"cell_type":"code","source":"data['English'] = data['English'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:35.635469Z","iopub.execute_input":"2024-09-19T09:17:35.635743Z","iopub.status.idle":"2024-09-19T09:17:35.692881Z","shell.execute_reply.started":"2024-09-19T09:17:35.635719Z","shell.execute_reply":"2024-09-19T09:17:35.692108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4.3'></a>\n### **3. Removing Html tags**","metadata":{"execution":{"iopub.status.busy":"2024-07-07T14:29:08.534207Z","iopub.execute_input":"2024-07-07T14:29:08.534756Z","iopub.status.idle":"2024-07-07T14:29:08.542960Z","shell.execute_reply.started":"2024-07-07T14:29:08.534718Z","shell.execute_reply":"2024-07-07T14:29:08.540966Z"}}},{"cell_type":"code","source":"import re\ndef remove_html_tags(text):\n    pattern = r'[^a-zA-Z0-9\\s]'\n    text = re.sub(pattern,'',text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:35.698218Z","iopub.execute_input":"2024-09-19T09:17:35.698561Z","iopub.status.idle":"2024-09-19T09:17:35.703305Z","shell.execute_reply.started":"2024-09-19T09:17:35.698535Z","shell.execute_reply":"2024-09-19T09:17:35.702285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['English'] = data['English'].apply(remove_html_tags)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:35.704458Z","iopub.execute_input":"2024-09-19T09:17:35.704737Z","iopub.status.idle":"2024-09-19T09:17:36.271120Z","shell.execute_reply.started":"2024-09-19T09:17:35.704714Z","shell.execute_reply":"2024-09-19T09:17:36.270122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4.4'></a>\n### **4.  Removing URLs**","metadata":{"execution":{"iopub.status.busy":"2024-07-07T14:37:08.592770Z","iopub.execute_input":"2024-07-07T14:37:08.593797Z","iopub.status.idle":"2024-07-07T14:37:08.602270Z","shell.execute_reply.started":"2024-07-07T14:37:08.593754Z","shell.execute_reply":"2024-07-07T14:37:08.600439Z"}}},{"cell_type":"code","source":"def remove_url(text):\n    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return pattern.sub(r'',text)\n\ndata['English'] = data['English'].apply(remove_url)\ndata['Hindi'] = data['Hindi'].apply(remove_url)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:36.272260Z","iopub.execute_input":"2024-09-19T09:17:36.272526Z","iopub.status.idle":"2024-09-19T09:17:37.102419Z","shell.execute_reply.started":"2024-09-19T09:17:36.272504Z","shell.execute_reply":"2024-09-19T09:17:37.101438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4.5'></a>\n### **5. Chat word treatment**","metadata":{}},{"cell_type":"code","source":"chat_words = {\n    \"AFAIK\": \"As Far As I Know\",\n    \"AFK\": \"Away From Keyboard\",\n    \"ASAP\": \"As Soon As Possible\",\n    \"ATK\": \"At The Keyboard\",\n    \"ATM\": \"At The Moment\",\n    \"A3\": \"Anytime, Anywhere, Anyplace\",\n    \"BAK\": \"Back At Keyboard\",\n    \"BBL\": \"Be Back Later\",\n    \"BBS\": \"Be Back Soon\",\n    \"BFN\": \"Bye For Now\",\n    \"B4N\": \"Bye For Now\",\n    \"BRB\": \"Be Right Back\",\n    \"BRT\": \"Be Right There\",\n    \"BTW\": \"By The Way\",\n    \"B4\": \"Before\",\n    \"CU\": \"See You\",\n    \"CUL8R\": \"See You Later\",\n    \"CYA\": \"See You\",\n    \"FAQ\": \"Frequently Asked Questions\",\n    \"FC\": \"Fingers Crossed\",\n    \"FWIW\": \"For What It's Worth\",\n    \"FYI\": \"For Your Information\",\n    \"GAL\": \"Get A Life\",\n    \"GG\": \"Good Game\",\n    \"GN\": \"Good Night\",\n    \"GMTA\": \"Great Minds Think Alike\",\n    \"GR8\": \"Great!\",\n    \"G9\": \"Genius\",\n    \"IC\": \"I See\",\n    \"ICQ\": \"I Seek you (also a chat program)\",\n    \"ILU\": \"I Love You\",\n    \"IMHO\": \"In My Honest/Humble Opinion\",\n    \"IMO\": \"In My Opinion\",\n    \"IOW\": \"In Other Words\",\n    \"IRL\": \"In Real Life\",\n    \"KISS\": \"Keep It Simple, Stupid\",\n    \"LDR\": \"Long Distance Relationship\",\n    \"LMAO\": \"Laugh My A.. Off\",\n    \"LOL\": \"Laughing Out Loud\",\n    \"LTNS\": \"Long Time No See\",\n    \"L8R\": \"Later\",\n    \"MTE\": \"My Thoughts Exactly\",\n    \"M8\": \"Mate\",\n    \"NRN\": \"No Reply Necessary\",\n    \"OIC\": \"Oh I See\",\n    \"PITA\": \"Pain In The A..\",\n    \"PRT\": \"Party\",\n    \"PRW\": \"Parents Are Watching\",\n    \"QPSA\": \"Que Pasa?\",\n    \"ROFL\": \"Rolling On The Floor Laughing\",\n    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n    \"SK8\": \"Skate\",\n    \"STATS\": \"Your sex and age\",\n    \"ASL\": \"Age, Sex, Location\",\n    \"THX\": \"Thank You\",\n    \"TTFN\": \"Ta-Ta For Now!\",\n    \"TTYL\": \"Talk To You Later\",\n    \"U\": \"You\",\n    \"U2\": \"You Too\",\n    \"U4E\": \"Yours For Ever\",\n    \"WB\": \"Welcome Back\",\n    \"WTF\": \"What The F...\",\n    \"WTG\": \"Way To Go!\",\n    \"WUF\": \"Where Are You From?\",\n    \"W8\": \"Wait...\",\n    \"7K\": \"Sick:-D Laughter\",\n    \"TFW\": \"That feeling when\",\n    \"MFW\": \"My face when\",\n    \"MRW\": \"My reaction when\",\n    \"IFYP\": \"I feel your pain\",\n    \"LOL\": \"Laughing out loud\",\n    \"TNTL\": \"Trying not to laugh\",\n    \"JK\": \"Just kidding\",\n    \"IDC\": \"I don‚Äôt care\",\n    \"ILY\": \"I love you\",\n    \"IMU\": \"I miss you\",\n    \"ADIH\": \"Another day in hell\",\n    \"IDC\": \"I don‚Äôt care\",\n    \"ZZZ\": \"Sleeping, bored, tired\",\n    \"WYWH\": \"Wish you were here\",\n    \"TIME\": \"Tears in my eyes\",\n    \"BAE\": \"Before anyone else\",\n    \"FIMH\": \"Forever in my heart\",\n    \"BSAAW\": \"Big smile and a wink\",\n    \"BWL\": \"Bursting with laughter\",\n    \"LMAO\": \"Laughing my a** off\",\n    \"BFF\": \"Best friends forever\",\n    \"CSL\": \"Can‚Äôt stop laughing\",\n}","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:37.103628Z","iopub.execute_input":"2024-09-19T09:17:37.103934Z","iopub.status.idle":"2024-09-19T09:17:37.116420Z","shell.execute_reply.started":"2024-09-19T09:17:37.103909Z","shell.execute_reply":"2024-09-19T09:17:37.115344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chat_conversion(text):\n    new_text=[]\n    for w in text.split():\n        if w.upper() in chat_words:\n            new_text.append(chat_words[w.upper()])\n        else:\n            new_text.append(w)\n    return \" \".join(new_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:37.117882Z","iopub.execute_input":"2024-09-19T09:17:37.118627Z","iopub.status.idle":"2024-09-19T09:17:37.130614Z","shell.execute_reply.started":"2024-09-19T09:17:37.118600Z","shell.execute_reply":"2024-09-19T09:17:37.129858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['English'] = data['English'].apply(chat_conversion)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:37.131546Z","iopub.execute_input":"2024-09-19T09:17:37.132098Z","iopub.status.idle":"2024-09-19T09:17:37.964294Z","shell.execute_reply.started":"2024-09-19T09:17:37.132074Z","shell.execute_reply":"2024-09-19T09:17:37.963318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4.6'></a>\n### **6. Removing Emojis**","metadata":{}},{"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:37.965453Z","iopub.execute_input":"2024-09-19T09:17:37.965760Z","iopub.status.idle":"2024-09-19T09:17:37.971710Z","shell.execute_reply.started":"2024-09-19T09:17:37.965736Z","shell.execute_reply":"2024-09-19T09:17:37.970873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['English'] = data['English'].apply(remove_emoji)\ndata['Hindi'] = data['Hindi'].apply(remove_emoji)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:37.973278Z","iopub.execute_input":"2024-09-19T09:17:37.973650Z","iopub.status.idle":"2024-09-19T09:17:39.792504Z","shell.execute_reply.started":"2024-09-19T09:17:37.973618Z","shell.execute_reply":"2024-09-19T09:17:39.791731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4.7'></a>\n### **7. Removing contraction**","metadata":{}},{"cell_type":"code","source":"def expand_contractions(text):\n    expanded_text = contractions.fix(text)\n    return expanded_text\n\ndata['English'] = data['English'].apply(expand_contractions)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:39.793576Z","iopub.execute_input":"2024-09-19T09:17:39.793884Z","iopub.status.idle":"2024-09-19T09:17:41.272569Z","shell.execute_reply.started":"2024-09-19T09:17:39.793857Z","shell.execute_reply":"2024-09-19T09:17:41.271788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4.8'></a>\n### **8. Remove Non-Hindi and Alphanumeric characters**","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text, language='english'):\n    if not isinstance(text, str):\n        return text\n    \n    if language == 'english':\n        pattern = re.compile(r'[^a-zA-Z0-9\\s]')\n        return pattern.sub(r'', text)\n    elif language == 'hindi':\n        pattern = re.compile(r'[^\\u0900-\\u097F\\s]')\n        return pattern.sub(r'', text)\n    else:\n        raise ValueError(\"Unsupported Language, Supported languages are 'english' and 'hindi'\")\n        \ndata['English'] = data['English'].apply(lambda x: preprocess_text(x, language='english'))\ndata['Hindi'] = data['Hindi'].apply(lambda x: preprocess_text(x, language='hindi'))","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:41.273560Z","iopub.execute_input":"2024-09-19T09:17:41.273852Z","iopub.status.idle":"2024-09-19T09:17:42.377183Z","shell.execute_reply.started":"2024-09-19T09:17:41.273813Z","shell.execute_reply":"2024-09-19T09:17:42.376174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:42.378673Z","iopub.execute_input":"2024-09-19T09:17:42.379010Z","iopub.status.idle":"2024-09-19T09:17:42.393364Z","shell.execute_reply.started":"2024-09-19T09:17:42.378985Z","shell.execute_reply":"2024-09-19T09:17:42.392371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4.9'></a>\n### **9. WordCloud**","metadata":{}},{"cell_type":"code","source":"# Function to preprocess text and generate word cloud\ndef generate_wordcloud(text_list, language):\n    # Join the list of words into a single string\n    text_str = ' '.join(map(str, text_list))\n    \n    # Generate word cloud without specifying font path\n    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text_str)\n    \n    # Display word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(f\"Word Cloud for {language} Text\")\n    plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:42.394432Z","iopub.execute_input":"2024-09-19T09:17:42.394705Z","iopub.status.idle":"2024-09-19T09:17:42.404004Z","shell.execute_reply.started":"2024-09-19T09:17:42.394681Z","shell.execute_reply":"2024-09-19T09:17:42.403198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate word clouds for English and Hindi columns\ngenerate_wordcloud(data['English'], 'english')","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:42.405118Z","iopub.execute_input":"2024-09-19T09:17:42.405411Z","iopub.status.idle":"2024-09-19T09:17:51.805634Z","shell.execute_reply.started":"2024-09-19T09:17:42.405362Z","shell.execute_reply":"2024-09-19T09:17:51.804689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:51.806745Z","iopub.execute_input":"2024-09-19T09:17:51.807055Z","iopub.status.idle":"2024-09-19T09:17:51.813324Z","shell.execute_reply.started":"2024-09-19T09:17:51.807030Z","shell.execute_reply":"2024-09-19T09:17:51.812162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data[data['English'].str.len() <= 50]","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:51.814501Z","iopub.execute_input":"2024-09-19T09:17:51.814893Z","iopub.status.idle":"2024-09-19T09:17:51.909998Z","shell.execute_reply.started":"2024-09-19T09:17:51.814862Z","shell.execute_reply":"2024-09-19T09:17:51.909045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:51.911119Z","iopub.execute_input":"2024-09-19T09:17:51.911379Z","iopub.status.idle":"2024-09-19T09:17:51.917010Z","shell.execute_reply.started":"2024-09-19T09:17:51.911358Z","shell.execute_reply":"2024-09-19T09:17:51.916078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add <start> and <end> tokens to Hindi sentences\ndata['Hindi'] = ['<start> ' + sentence + ' <end>' for sentence in data['Hindi']]","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:51.918164Z","iopub.execute_input":"2024-09-19T09:17:51.918429Z","iopub.status.idle":"2024-09-19T09:17:51.955543Z","shell.execute_reply.started":"2024-09-19T09:17:51.918406Z","shell.execute_reply":"2024-09-19T09:17:51.954703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4.10'></a>\n### **10. Tokenize on the Data**","metadata":{}},{"cell_type":"code","source":"tok = Tokenizer()\ntok.fit_on_texts(data['English'])","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:51.956725Z","iopub.execute_input":"2024-09-19T09:17:51.957183Z","iopub.status.idle":"2024-09-19T09:17:52.681333Z","shell.execute_reply.started":"2024-09-19T09:17:51.957156Z","shell.execute_reply":"2024-09-19T09:17:52.680518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok_hindi = Tokenizer()\ntok_hindi.fit_on_texts(data['Hindi'])","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:52.682533Z","iopub.execute_input":"2024-09-19T09:17:52.683233Z","iopub.status.idle":"2024-09-19T09:17:53.685010Z","shell.execute_reply.started":"2024-09-19T09:17:52.683198Z","shell.execute_reply":"2024-09-19T09:17:53.684000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tok.word_index),len(tok_hindi.word_index)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:53.686255Z","iopub.execute_input":"2024-09-19T09:17:53.686615Z","iopub.status.idle":"2024-09-19T09:17:53.693566Z","shell.execute_reply.started":"2024-09-19T09:17:53.686582Z","shell.execute_reply":"2024-09-19T09:17:53.692696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tok.word_index),len(tok_hindi.word_index)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:53.694861Z","iopub.execute_input":"2024-09-19T09:17:53.695277Z","iopub.status.idle":"2024-09-19T09:17:53.705026Z","shell.execute_reply.started":"2024-09-19T09:17:53.695246Z","shell.execute_reply":"2024-09-19T09:17:53.704058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok.document_count","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:53.706229Z","iopub.execute_input":"2024-09-19T09:17:53.706791Z","iopub.status.idle":"2024-09-19T09:17:53.715571Z","shell.execute_reply.started":"2024-09-19T09:17:53.706767Z","shell.execute_reply":"2024-09-19T09:17:53.714844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Converting data in numerical forms**","metadata":{}},{"cell_type":"code","source":"data['English'] = tok.texts_to_sequences(data['English'])\ndata['Hindi'] = tok_hindi.texts_to_sequences(data['Hindi'])","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:53.716619Z","iopub.execute_input":"2024-09-19T09:17:53.717048Z","iopub.status.idle":"2024-09-19T09:17:55.460346Z","shell.execute_reply.started":"2024-09-19T09:17:53.717017Z","shell.execute_reply":"2024-09-19T09:17:55.459499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:55.461566Z","iopub.execute_input":"2024-09-19T09:17:55.462224Z","iopub.status.idle":"2024-09-19T09:17:55.481084Z","shell.execute_reply.started":"2024-09-19T09:17:55.462190Z","shell.execute_reply":"2024-09-19T09:17:55.480150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_max_sequence_length(eng,hindi):\n    max_length_combined = max(max(len(seq) for seq in eng), max(len(seq) for seq in hindi))\n\n    return max_length_combined\n\nmax_length_combined = calculate_max_sequence_length(data['English'],data['Hindi'])\nmax_length_combined","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:55.482389Z","iopub.execute_input":"2024-09-19T09:17:55.482647Z","iopub.status.idle":"2024-09-19T09:17:55.513002Z","shell.execute_reply.started":"2024-09-19T09:17:55.482625Z","shell.execute_reply":"2024-09-19T09:17:55.512145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for num in data['English'][0]:\n    print(num, end=' ')","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:55.514210Z","iopub.execute_input":"2024-09-19T09:17:55.514588Z","iopub.status.idle":"2024-09-19T09:17:55.523013Z","shell.execute_reply.started":"2024-09-19T09:17:55.514557Z","shell.execute_reply":"2024-09-19T09:17:55.522017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print corresponding words horizontally\nfor num in data['English'][0]:\n    word = tok.index_word.get(num, 'UNK')  \n    print(word, end=' ')","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:55.524313Z","iopub.execute_input":"2024-09-19T09:17:55.524723Z","iopub.status.idle":"2024-09-19T09:17:55.532150Z","shell.execute_reply.started":"2024-09-19T09:17:55.524690Z","shell.execute_reply":"2024-09-19T09:17:55.531229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for num in data['Hindi'][0]:\n    word = tok_hindi.index_word.get(num)\n    print(word,end = \" \")","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:55.539370Z","iopub.execute_input":"2024-09-19T09:17:55.539684Z","iopub.status.idle":"2024-09-19T09:17:55.544670Z","shell.execute_reply.started":"2024-09-19T09:17:55.539661Z","shell.execute_reply":"2024-09-19T09:17:55.543718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4.11'></a>\n### **11. separating the data in dependent and independent**","metadata":{}},{"cell_type":"code","source":"hindi = data['Hindi']\nenglish = data['English']","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:55.545777Z","iopub.execute_input":"2024-09-19T09:17:55.546055Z","iopub.status.idle":"2024-09-19T09:17:55.552698Z","shell.execute_reply.started":"2024-09-19T09:17:55.546033Z","shell.execute_reply":"2024-09-19T09:17:55.551770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4.12'></a>\n### **12. Padding**","metadata":{}},{"cell_type":"code","source":"x = pad_sequences(english,maxlen = max_length_combined,padding = 'post')\ny = pad_sequences(hindi,maxlen = max_length_combined,padding = 'post')","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:55.553783Z","iopub.execute_input":"2024-09-19T09:17:55.554088Z","iopub.status.idle":"2024-09-19T09:17:55.998743Z","shell.execute_reply.started":"2024-09-19T09:17:55.554065Z","shell.execute_reply":"2024-09-19T09:17:55.997964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:55.999764Z","iopub.execute_input":"2024-09-19T09:17:56.000045Z","iopub.status.idle":"2024-09-19T09:17:56.006201Z","shell.execute_reply.started":"2024-09-19T09:17:56.000022Z","shell.execute_reply":"2024-09-19T09:17:56.005318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:56.007481Z","iopub.execute_input":"2024-09-19T09:17:56.007856Z","iopub.status.idle":"2024-09-19T09:17:56.018102Z","shell.execute_reply.started":"2024-09-19T09:17:56.007804Z","shell.execute_reply":"2024-09-19T09:17:56.017164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4.13'></a>\n### **13. splitting in train test and val**","metadata":{"execution":{"iopub.status.busy":"2024-07-08T06:49:45.809649Z","iopub.execute_input":"2024-07-08T06:49:45.810260Z","iopub.status.idle":"2024-07-08T06:49:45.820790Z","shell.execute_reply.started":"2024-07-08T06:49:45.810223Z","shell.execute_reply":"2024-07-08T06:49:45.818713Z"}}},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:56.019238Z","iopub.execute_input":"2024-09-19T09:17:56.019640Z","iopub.status.idle":"2024-09-19T09:17:56.069207Z","shell.execute_reply.started":"2024-09-19T09:17:56.019608Z","shell.execute_reply":"2024-09-19T09:17:56.068380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the shape of the resulting arrays\nprint(\"Shape of x_train:\", x_train.shape)\nprint(\"Shape of x_test:\", x_test.shape)\nprint(\"Shape of y_train:\", y_train.shape)\nprint(\"Shape of y_test:\", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:56.070205Z","iopub.execute_input":"2024-09-19T09:17:56.070465Z","iopub.status.idle":"2024-09-19T09:17:56.075723Z","shell.execute_reply.started":"2024-09-19T09:17:56.070443Z","shell.execute_reply":"2024-09-19T09:17:56.074848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#78D1E1;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n\n<a id='5'></a>\n# 5 | Modelling\n\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;\"></div>\n","metadata":{}},{"cell_type":"markdown","source":"<a id='5.1'></a>\n### **5.1. ENCODER-DECODER MODEL**","metadata":{"execution":{"iopub.status.busy":"2024-07-12T06:02:20.301957Z","iopub.execute_input":"2024-07-12T06:02:20.302442Z","iopub.status.idle":"2024-07-12T06:02:20.309605Z","shell.execute_reply.started":"2024-07-12T06:02:20.302394Z","shell.execute_reply":"2024-07-12T06:02:20.308069Z"}}},{"cell_type":"code","source":"max_length_input = x_train.shape[1]\nmax_length_output = y_train.shape[1]\ninput_vocab_size = len(tok.word_index) + 1\noutput_vocab_size = len(tok_hindi.word_index) + 1\n\n# Define Encoder model\nencoder_inputs = Input(shape=(max_length_input,))\nencoder_embedding = Embedding(input_dim=input_vocab_size, output_dim=260)(encoder_inputs)\nencoder_lstm = LSTM(156, return_state=True)\nencoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\nencoder_states = [state_h, state_c]\n\n# Define Decoder model\ndecoder_inputs = Input(shape=(max_length_output,))\ndecoder_embedding = Embedding(input_dim=output_vocab_size, output_dim=260)(decoder_inputs)\ndecoder_lstm = LSTM(156, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\ndecoder_dense = Dense(output_vocab_size, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the full Encoder-Decoder model\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Print model summary\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:56.077147Z","iopub.execute_input":"2024-09-19T09:17:56.077493Z","iopub.status.idle":"2024-09-19T09:17:57.322997Z","shell.execute_reply.started":"2024-09-19T09:17:56.077463Z","shell.execute_reply":"2024-09-19T09:17:57.322095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\nmodel_checkpoint = ModelCheckpoint('model_checkpoint.keras', save_best_only=True)  # Updated filepath\n\n# Train the model with callbacks\nhistory = model.fit(\n    x=[x_train, y_train],# Exclude last token from decoder input\n    y=y_train,   # Exclude first token from decoder target\n    batch_size=32,\n    epochs=5,\n    validation_data=([x_test, y_test], y_test),\n    callbacks=[early_stopping, model_checkpoint],  \n)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:17:57.324010Z","iopub.execute_input":"2024-09-19T09:17:57.324299Z","iopub.status.idle":"2024-09-19T09:23:32.287372Z","shell.execute_reply.started":"2024-09-19T09:17:57.324275Z","shell.execute_reply":"2024-09-19T09:23:32.286583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.2'></a>\n### **5.2. ENCODER-DECODER MODEL EVALUATION**","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history['accuracy'],color = 'blue',label = 'accuracy')\nplt.plot(history.history['val_accuracy'],color = 'red',label = 'val_accuracy')\nplt.title(\"Training And Validation Accuracy Score\")\nplt.xlabel(\"accuracy\")\nplt.ylabel(\"epochs\")\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:21:04.168338Z","iopub.execute_input":"2024-09-19T08:21:04.168669Z","iopub.status.idle":"2024-09-19T08:21:04.500494Z","shell.execute_reply.started":"2024-09-19T08:21:04.168642Z","shell.execute_reply":"2024-09-19T08:21:04.499642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'],color = 'blue',label = 'loss')\nplt.plot(history.history['val_loss'],color = 'red',label = 'val_loss')\nplt.title(\"Training And Validation Loss Score\")\nplt.xlabel(\"loss\")\nplt.ylabel(\"epochs\")\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:21:04.501746Z","iopub.execute_input":"2024-09-19T08:21:04.502106Z","iopub.status.idle":"2024-09-19T08:21:04.776725Z","shell.execute_reply.started":"2024-09-19T08:21:04.502073Z","shell.execute_reply":"2024-09-19T08:21:04.775790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.3'></a>\n### **5.3. PREDICTION FROM ENCODER-DECODER MODEL**","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:03:22.474535Z","iopub.execute_input":"2024-07-17T17:03:22.474977Z","iopub.status.idle":"2024-07-17T17:03:22.479319Z","shell.execute_reply.started":"2024-07-17T17:03:22.474942Z","shell.execute_reply":"2024-07-17T17:03:22.478452Z"}}},{"cell_type":"code","source":"import numpy as np\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n\n# Reverse token dictionaries\nrev_tok_hindi = {idx: word for word, idx in tok_hindi.word_index.items()}\n\n# Select a subset of test data\nnum_samples = 10\nx_test_subset = x_test[:num_samples]\ny_test_padded_subset = y_test[:num_samples]\n\n# Predict\npredictions = model.predict([x_test_subset, y_test_padded_subset], batch_size=16)\nprint('Shape of predictions:', predictions.shape)\n\n# Convert predictions to token indices\npredicted_tokens_np = np.argmax(predictions, axis=-1)\nprint('Shape of predicted_tokens:', predicted_tokens_np.shape)\n\n# Map indices to tokens and remove 'start' and 'end' tokens\npredicted_sentences = []\nfor sample in predicted_tokens_np:\n    sentence = ' '.join([rev_tok_hindi.get(token, '<unknown>') for token in sample if token != 0 and token not in [tok_hindi.word_index.get('start'), tok_hindi.word_index.get('end')]])  # Exclude padding, 'start', and 'end' tokens\n    predicted_sentences.append(sentence)\n\n# Reverse token dictionary for English (assuming you have `tok_english` for English tokens)\nrev_tok_english = {idx: word for word, idx in tok.word_index.items()}\n\n# Map the English test inputs to their corresponding sentences\nenglish_sentences = []\nfor sample in x_test_subset:\n    sentence = ' '.join([rev_tok_english.get(token, '<unknown>') for token in sample if token != 0])  # Exclude padding token\n    english_sentences.append(sentence)\n\n# Print English sentence followed by predicted Hindi sentence\nfor idx, (eng_sentence, hin_sentence) in enumerate(zip(english_sentences, predicted_sentences)):\n    print(f'English sentence {idx + 1}: {eng_sentence}')\n    print(f'Predicted Hindi translation {idx + 1}: {hin_sentence}\\n')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:21:04.778430Z","iopub.execute_input":"2024-09-19T08:21:04.778703Z","iopub.status.idle":"2024-09-19T08:21:05.371470Z","shell.execute_reply.started":"2024-09-19T08:21:04.778679Z","shell.execute_reply":"2024-09-19T08:21:05.370466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.4'></a>\n### **5.4. ENCODER-DECODER MODEL WITH ATTENTION**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Attention, Concatenate\nfrom tensorflow.keras.models import Model\n\n# Define Encoder model\nencoder_inputs = Input(shape=(max_length_input,))\nencoder_embedding = Embedding(input_dim=input_vocab_size, output_dim=260)(encoder_inputs)\nencoder_lstm = LSTM(156, return_sequences=True, return_state=True)  # Ensure return_sequences=True\nencoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\nencoder_states = [state_h, state_c]\n\n# Define Decoder model\ndecoder_inputs = Input(shape=(max_length_output,))\ndecoder_embedding = Embedding(input_dim=output_vocab_size, output_dim=260)(decoder_inputs)\ndecoder_lstm = LSTM(156, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n\n# Attention Layer\nattention = Attention()([decoder_outputs, encoder_outputs])\n\n# Concatenate attention output with decoder outputs\ndecoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention])\n\n# Dense Layer for output prediction\ndecoder_dense = Dense(output_vocab_size, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_concat_input)\n\n# Define the full Encoder-Decoder model\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Print model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:25:56.751351Z","iopub.execute_input":"2024-09-19T08:25:56.752126Z","iopub.status.idle":"2024-09-19T08:25:56.853719Z","shell.execute_reply.started":"2024-09-19T08:25:56.752093Z","shell.execute_reply":"2024-09-19T08:25:56.852895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\nmodel_checkpoint = ModelCheckpoint('model_checkpoint.keras', save_best_only=True)  # Updated filepath\n\n# Train the model with callbacks\nhistory = model.fit(\n    x=[x_train, y_train],# Exclude last token from decoder input\n    y=y_train,   # Exclude first token from decoder target\n    batch_size=32,\n    epochs=5,\n    validation_data=([x_test, y_test], y_test),\n    callbacks=[early_stopping, model_checkpoint],  \n)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:26:17.516180Z","iopub.execute_input":"2024-09-19T08:26:17.517428Z","iopub.status.idle":"2024-09-19T09:00:52.782242Z","shell.execute_reply.started":"2024-09-19T08:26:17.517391Z","shell.execute_reply":"2024-09-19T09:00:52.781396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.5'></a>\n### **5.5. ENCODER-DECODER MODEL WITH ATTENTION EVALUATION**","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history['accuracy'],color = 'blue',label = 'accuracy')\nplt.plot(history.history['val_accuracy'],color = 'red',label = 'val_accuracy')\nplt.title(\"Training And Validation Accuracy Score\")\nplt.xlabel(\"accuracy\")\nplt.ylabel(\"epochs\")\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:01:11.681751Z","iopub.execute_input":"2024-09-19T09:01:11.682602Z","iopub.status.idle":"2024-09-19T09:01:12.015070Z","shell.execute_reply.started":"2024-09-19T09:01:11.682568Z","shell.execute_reply":"2024-09-19T09:01:12.013995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'],color = 'blue',label = 'loss')\nplt.plot(history.history['val_loss'],color = 'red',label = 'val_loss')\nplt.title(\"Training And Validation Loss Score\")\nplt.xlabel(\"loss\")\nplt.ylabel(\"epochs\")\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:01:12.402685Z","iopub.execute_input":"2024-09-19T09:01:12.402990Z","iopub.status.idle":"2024-09-19T09:01:12.732003Z","shell.execute_reply.started":"2024-09-19T09:01:12.402965Z","shell.execute_reply":"2024-09-19T09:01:12.731075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.6'></a>\n### **5.6. PREDICTION FROM ENCODER-DECODER MODEL WITH ATTENTION**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n\n# Reverse token dictionaries\nrev_tok_hindi = {idx: word for word, idx in tok_hindi.word_index.items()}\n\n# Select a subset of test data\nnum_samples = 10\nx_test_subset = x_test[:num_samples]\ny_test_padded_subset = y_test[:num_samples]\n\n# Predict\npredictions = model.predict([x_test_subset, y_test_padded_subset], batch_size=16)\nprint('Shape of predictions:', predictions.shape)\n\n# Convert predictions to token indices\npredicted_tokens_np = np.argmax(predictions, axis=-1)\nprint('Shape of predicted_tokens:', predicted_tokens_np.shape)\n\n# Map indices to tokens and remove 'start' and 'end' tokens\npredicted_sentences = []\nfor sample in predicted_tokens_np:\n    sentence = ' '.join([rev_tok_hindi.get(token, '<unknown>') for token in sample if token != 0 and token not in [tok_hindi.word_index.get('start'), tok_hindi.word_index.get('end')]])  # Exclude padding, 'start', and 'end' tokens\n    predicted_sentences.append(sentence)\n\n# Reverse token dictionary for English (assuming you have `tok_english` for English tokens)\nrev_tok_english = {idx: word for word, idx in tok.word_index.items()}\n\n# Map the English test inputs to their corresponding sentences\nenglish_sentences = []\nfor sample in x_test_subset:\n    sentence = ' '.join([rev_tok_english.get(token, '<unknown>') for token in sample if token != 0])  # Exclude padding token\n    english_sentences.append(sentence)\n\n# Print English sentence followed by predicted Hindi sentence\nfor idx, (eng_sentence, hin_sentence) in enumerate(zip(english_sentences, predicted_sentences)):\n    print(f'English sentence {idx + 1}: {eng_sentence}')\n    print(f'Predicted Hindi translation {idx + 1}: {hin_sentence}\\n')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:01:14.034653Z","iopub.execute_input":"2024-09-19T09:01:14.035418Z","iopub.status.idle":"2024-09-19T09:01:14.680621Z","shell.execute_reply.started":"2024-09-19T09:01:14.035388Z","shell.execute_reply":"2024-09-19T09:01:14.679607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.7'></a>\n### **5.7. Saving the best weight**","metadata":{}},{"cell_type":"code","source":"# Save the entire model\nmodel.save('encoder_decoder_model.h5')\nprint('Model saved succesfully!!')","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:23:36.515706Z","iopub.execute_input":"2024-09-19T09:23:36.516089Z","iopub.status.idle":"2024-09-19T09:23:36.807806Z","shell.execute_reply.started":"2024-09-19T09:23:36.516060Z","shell.execute_reply":"2024-09-19T09:23:36.806924Z"},"trusted":true},"execution_count":null,"outputs":[]}]}